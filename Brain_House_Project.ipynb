{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCYNS8Oivv4R"
      },
      "outputs": [],
      "source": [
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easyocr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRqcmozvxZc3",
        "outputId": "29c4f3ee-ab49-4395-a4a7-def9831724fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.0+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.23.2)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2024.6.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.8.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n",
            "Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n",
            "Successfully installed easyocr-1.7.1 ninja-1.11.1.1 pyclipper-1.3.0.post5 python-bidi-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7oq6Z1Sxcdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "import easyocr\n",
        "\n",
        "# Function to load data from image and annotations\n",
        "def load_data(image_dir, annotation_dir):\n",
        "    annotations = []\n",
        "\n",
        "    for json_file in os.listdir(annotation_dir):\n",
        "        if json_file.endswith('.json'):\n",
        "            json_path = os.path.join(annotation_dir, json_file)\n",
        "            with open(json_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "                for img_key, img_data in data.items():\n",
        "                    image_path = os.path.join(image_dir, img_data[\"filename\"])\n",
        "                    image = cv2.imread(image_path)\n",
        "\n",
        "                    if image is None:\n",
        "                        continue\n",
        "\n",
        "                    for region in img_data.get('regions', []):\n",
        "                        shape_attr = region['shape_attributes']\n",
        "                        region_attr = region['region_attributes']\n",
        "\n",
        "                        x, y, w, h = shape_attr['x'], shape_attr['y'], shape_attr['width'], shape_attr['height']\n",
        "                        text = region_attr['text']\n",
        "                        field = region_attr['field']\n",
        "\n",
        "                        annotations.append({\n",
        "                            'image_path': image_path,\n",
        "                            'bbox': (x, y, w, h),\n",
        "                            'field': field,\n",
        "                            'text': text\n",
        "                        })\n",
        "\n",
        "    return pd.DataFrame(annotations)\n",
        "\n",
        "# Load your data\n",
        "image_dir = '/content/drive/MyDrive/Created_Dataset/DATA'\n",
        "annotation_dir = '/content/drive/MyDrive/Created_Dataset/Annotation'\n",
        "data_df = load_data(image_dir, annotation_dir)\n",
        "\n",
        "# Prepare data for model\n",
        "def prepare_data(df):\n",
        "    images = []\n",
        "    texts = []\n",
        "    fields = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        image = cv2.imread(row['image_path'])\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, (64, 64))  # Resize to a fixed size for training\n",
        "            images.append(image)\n",
        "            texts.append(row['text'])\n",
        "            fields.append(row['field'])\n",
        "\n",
        "    images = np.array(images)\n",
        "\n",
        "    return images, texts, fields\n",
        "\n",
        "images, texts, fields = prepare_data(data_df)\n",
        "\n",
        "# Encode the labels (fields)\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(fields)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "labels_onehot = onehot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "# Tokenize text data\n",
        "text_tokenizer = Tokenizer(num_words=5000)\n",
        "text_tokenizer.fit_on_texts(texts)\n",
        "text_sequences = text_tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Define a maxlen for the sequences\n",
        "maxlen = 100\n",
        "text_data = pad_sequences(text_sequences, maxlen=maxlen)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test, text_train, text_test = train_test_split(\n",
        "    images, labels_onehot, text_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to create and train the LSTM-based text classifier\n",
        "def train_text_classifier(texts, labels, maxlen):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=128, input_length=maxlen),  # Embedding layer for text input\n",
        "        LSTM(128, return_sequences=False),  # LSTM layer for sequence data\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(labels.shape[1], activation='softmax')  # Number of classes from labels\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(texts, labels, epochs=10, validation_split=0.2)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "text_classifier_model = train_text_classifier(text_train, y_train, maxlen)\n",
        "\n",
        "# Save the model for later use\n",
        "text_classifier_model.save('text_classifier_model.h5')\n",
        "\n",
        "# Use EasyOCR to extract text from an image\n",
        "def detect_and_extract_text_alternative(image_path):\n",
        "    reader = easyocr.Reader(['en'])  # Initialize EasyOCR reader with English\n",
        "    results = reader.readtext(image_path)\n",
        "\n",
        "    texts = [result[1] for result in results]\n",
        "    return texts\n",
        "\n",
        "# Function to classify extracted texts\n",
        "def classify_texts(texts, model, tokenizer, maxlen):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=maxlen)  # Pad sequences to maxlen\n",
        "\n",
        "    predictions = model.predict(padded_sequences)\n",
        "    class_indices = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Assuming you have a label encoder to map indices to class names\n",
        "    field_names = label_encoder.classes_\n",
        "    classified_fields = [field_names[idx] for idx in class_indices]\n",
        "\n",
        "    return classified_fields\n",
        "\n",
        "# Process an image, extract text, and classify it\n",
        "def process_image(image_path, model, tokenizer, maxlen):\n",
        "    # Extract text from the image\n",
        "    extracted_texts = detect_and_extract_text_alternative(image_path)\n",
        "\n",
        "    # Classify the extracted texts\n",
        "    classified_fields = classify_texts(extracted_texts, model, tokenizer, maxlen)\n",
        "\n",
        "    # Combine the extracted texts with their corresponding classifications\n",
        "    results = list(zip(extracted_texts, classified_fields))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage with an image\n",
        "image_path = '/content/drive/MyDrive/Created_Dataset/SS.png'\n",
        "results = process_image(image_path, text_classifier_model, text_tokenizer, maxlen)\n",
        "\n",
        "# Print the results\n",
        "print(\"Results:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkNn5cVwv4rh",
        "outputId": "4a0a6a2d-13b6-4d2b-9434-52156acf25eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 486ms/step - accuracy: 0.1692 - loss: 2.0742 - val_accuracy: 0.3000 - val_loss: 2.0154\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step - accuracy: 0.3900 - loss: 1.9793 - val_accuracy: 0.3000 - val_loss: 1.8188\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - accuracy: 0.4110 - loss: 1.7115 - val_accuracy: 0.3000 - val_loss: 1.5252\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258ms/step - accuracy: 0.4017 - loss: 1.3764 - val_accuracy: 0.2000 - val_loss: 1.5562\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step - accuracy: 0.3598 - loss: 1.2769 - val_accuracy: 0.2000 - val_loss: 1.5448\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.2475 - loss: 1.2716 - val_accuracy: 0.3000 - val_loss: 1.5478\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.3627 - loss: 1.2306 - val_accuracy: 0.3000 - val_loss: 1.6026\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.3783 - loss: 1.2587 - val_accuracy: 0.3000 - val_loss: 1.5924\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.4174 - loss: 1.1950 - val_accuracy: 0.5000 - val_loss: 1.5605\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.6766 - loss: 1.1591 - val_accuracy: 0.6000 - val_loss: 1.5142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
            "/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
            "Results: [('FAUGET', 'Student_Name'), ('HiGh', 'Student_Name'), ('SCHOOL', 'Student_Name'), ('STUDENT CARD', 'Student_Name'), ('Student Name', 'Student_Name'), ('Kyrie Petrakis', 'Student_Name'), ('Student ID', 'Student_Name'), ('123-456-7890', 'Student_Name'), ('D.O.B', 'Student_Name'), ('12/5/2000', 'Student_Name'), ('Home', 'Student_Name'), ('123 Anywhere St.,', 'Student_Name'), ('Address', 'Student_Name'), ('City', 'Student_Name'), ('In17 &', 'Student_Name'), ('Princlpal Name', 'Student_Name'), ('Any', 'Student_Name')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqbiVI2OwEVt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sRnug_bhyR3i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}